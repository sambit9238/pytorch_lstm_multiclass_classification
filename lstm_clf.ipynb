{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/drugsComTest_train.csv')\n",
    "df_test = pd.read_csv('data/drugsComTest_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"I&amp;#039;ve been taking it for a few years, so ...</td>\n",
       "      <td>Birth Control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review      condition\n",
       "0  \"I&#039;ve been taking it for a few years, so ...  Birth Control"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Unfortunetly abilify didn&amp;#039;t work for me,...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   condition\n",
       "0  \"Unfortunetly abilify didn&#039;t work for me,...  Depression"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can experiment with more processing\n",
    "import re\n",
    "df_train['review'] = df_train['review'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()))\n",
    "df_test['review'] = df_test['review'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(df_train['condition'].values)\n",
    "test_y = le.transform(df_test['condition'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = prepare_vocab(df_train['review'])\n",
    "w2id_dict = prepare_word_dict(vocab_list)\n",
    "id2w_dict = {i:w for w, i in w2id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sents_train = sent_to_idx(df_train['review'], w2id_dict)\n",
    "padded_sents_test = sent_to_idx(df_test['review'], w2id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamsam/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers=1, drop_p = 0.8):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab  \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        #concat of avg_pool and max_pool generate twice the dimension\n",
    "        self.fc = nn.Linear(n_hidden*2, n_hidden//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(n_hidden//2, n_output) \n",
    "        \n",
    "    def forward(self, tokenized_idx):\n",
    "        embeddings = self.embedding(tokenized_idx)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        avg_pool = torch.mean(lstm_out, 1)\n",
    "        max_pool, _ = torch.max(lstm_out, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.fc(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
       "       'Depression', 'Pain'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "n_vocab = len(w2id_dict)\n",
    "n_embed = 64\n",
    "n_hidden = 100\n",
    "n_output = 6\n",
    "n_layers = 2\n",
    "batch_size = 100\n",
    "\n",
    "model = LSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "x_train = torch.tensor(padded_sents_train, dtype=torch.long).cuda()\n",
    "y_train = torch.tensor(train_y, dtype=torch.long).cuda()\n",
    "x_test = torch.tensor(padded_sents_test, dtype=torch.long).cuda()\n",
    "y_test = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3809/551478279.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_preds[i * batch_size:(i+1) * batch_size] = nn.functional.softmax(y_pred).cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 \t loss=68.3196 \t val_loss=77.5651  \t val_acc=0.7349  \t time=1.58s\n",
      "Epoch 2/100 \t loss=63.7386 \t val_loss=71.3206  \t val_acc=0.7364  \t time=1.33s\n",
      "Epoch 3/100 \t loss=62.0838 \t val_loss=69.9812  \t val_acc=0.7379  \t time=1.33s\n",
      "Epoch 4/100 \t loss=60.9789 \t val_loss=66.9399  \t val_acc=0.7640  \t time=1.34s\n",
      "Epoch 5/100 \t loss=56.8759 \t val_loss=77.6947  \t val_acc=0.7610  \t time=1.33s\n",
      "Epoch 6/100 \t loss=53.7744 \t val_loss=76.4093  \t val_acc=0.7667  \t time=1.31s\n",
      "Epoch 7/100 \t loss=52.0550 \t val_loss=72.7950  \t val_acc=0.7737  \t time=1.32s\n",
      "Epoch 8/100 \t loss=54.6803 \t val_loss=65.6027  \t val_acc=0.7587  \t time=1.31s\n",
      "Epoch 9/100 \t loss=55.0412 \t val_loss=61.5821  \t val_acc=0.7750  \t time=1.33s\n",
      "Epoch 10/100 \t loss=51.0044 \t val_loss=65.5553  \t val_acc=0.8025  \t time=1.34s\n",
      "Epoch 11/100 \t loss=48.9758 \t val_loss=69.5631  \t val_acc=0.7927  \t time=1.32s\n",
      "Epoch 12/100 \t loss=46.3535 \t val_loss=66.2062  \t val_acc=0.8068  \t time=1.33s\n",
      "Epoch 13/100 \t loss=47.0988 \t val_loss=73.7277  \t val_acc=0.8128  \t time=1.33s\n",
      "Epoch 14/100 \t loss=46.3389 \t val_loss=76.8606  \t val_acc=0.8190  \t time=1.33s\n",
      "Epoch 15/100 \t loss=44.5194 \t val_loss=71.0879  \t val_acc=0.8203  \t time=1.36s\n",
      "Epoch 16/100 \t loss=43.8686 \t val_loss=64.8723  \t val_acc=0.8238  \t time=1.32s\n",
      "Epoch 17/100 \t loss=44.5204 \t val_loss=77.8587  \t val_acc=0.8355  \t time=1.32s\n",
      "Epoch 18/100 \t loss=42.3109 \t val_loss=75.7206  \t val_acc=0.8338  \t time=1.32s\n",
      "Epoch 19/100 \t loss=42.9469 \t val_loss=82.8525  \t val_acc=0.8320  \t time=1.35s\n",
      "Epoch 20/100 \t loss=40.8189 \t val_loss=71.6014  \t val_acc=0.8401  \t time=1.32s\n",
      "Epoch 21/100 \t loss=40.0362 \t val_loss=103.9136  \t val_acc=0.8375  \t time=1.32s\n",
      "Epoch 22/100 \t loss=38.5495 \t val_loss=82.9570  \t val_acc=0.8375  \t time=1.32s\n",
      "Epoch 23/100 \t loss=39.2601 \t val_loss=89.9701  \t val_acc=0.8368  \t time=1.36s\n",
      "Epoch 24/100 \t loss=37.6845 \t val_loss=90.9833  \t val_acc=0.8451  \t time=1.34s\n",
      "Epoch 25/100 \t loss=35.7560 \t val_loss=110.1825  \t val_acc=0.8411  \t time=1.33s\n",
      "Epoch 26/100 \t loss=36.7702 \t val_loss=96.6248  \t val_acc=0.8338  \t time=1.33s\n",
      "Epoch 27/100 \t loss=38.0176 \t val_loss=73.7828  \t val_acc=0.8486  \t time=1.32s\n",
      "Epoch 28/100 \t loss=37.9661 \t val_loss=90.5490  \t val_acc=0.8383  \t time=1.33s\n",
      "Epoch 29/100 \t loss=35.3579 \t val_loss=80.9389  \t val_acc=0.8431  \t time=1.35s\n",
      "Epoch 30/100 \t loss=35.8230 \t val_loss=96.3447  \t val_acc=0.8406  \t time=1.33s\n",
      "Epoch 31/100 \t loss=34.5632 \t val_loss=138.5131  \t val_acc=0.8395  \t time=1.35s\n",
      "Epoch 32/100 \t loss=34.6001 \t val_loss=120.8042  \t val_acc=0.8403  \t time=1.34s\n",
      "Epoch 33/100 \t loss=34.9497 \t val_loss=92.8529  \t val_acc=0.8398  \t time=1.35s\n",
      "Epoch 34/100 \t loss=34.0326 \t val_loss=97.4561  \t val_acc=0.8468  \t time=1.36s\n",
      "Epoch 35/100 \t loss=32.9496 \t val_loss=95.5395  \t val_acc=0.8406  \t time=1.35s\n",
      "Epoch 36/100 \t loss=33.2182 \t val_loss=106.8393  \t val_acc=0.8453  \t time=1.34s\n",
      "Epoch 37/100 \t loss=31.0029 \t val_loss=118.8369  \t val_acc=0.8481  \t time=1.32s\n",
      "Epoch 38/100 \t loss=32.2200 \t val_loss=84.0855  \t val_acc=0.8448  \t time=1.35s\n",
      "Epoch 39/100 \t loss=31.9497 \t val_loss=104.2907  \t val_acc=0.8443  \t time=1.33s\n",
      "Epoch 40/100 \t loss=30.5794 \t val_loss=139.0977  \t val_acc=0.8433  \t time=1.36s\n",
      "Epoch 41/100 \t loss=29.6302 \t val_loss=131.6968  \t val_acc=0.8436  \t time=1.33s\n",
      "Epoch 42/100 \t loss=29.2437 \t val_loss=155.6315  \t val_acc=0.8453  \t time=1.34s\n",
      "Epoch 43/100 \t loss=31.2204 \t val_loss=153.8270  \t val_acc=0.8378  \t time=1.33s\n",
      "Epoch 44/100 \t loss=30.9031 \t val_loss=116.7702  \t val_acc=0.8481  \t time=1.32s\n",
      "Epoch 45/100 \t loss=29.3979 \t val_loss=116.1060  \t val_acc=0.8496  \t time=1.33s\n",
      "Epoch 46/100 \t loss=27.7160 \t val_loss=127.4255  \t val_acc=0.8521  \t time=1.34s\n",
      "Epoch 47/100 \t loss=27.5798 \t val_loss=113.9975  \t val_acc=0.8375  \t time=1.33s\n",
      "Epoch 48/100 \t loss=27.5477 \t val_loss=158.7150  \t val_acc=0.8453  \t time=1.32s\n",
      "Epoch 49/100 \t loss=29.3074 \t val_loss=100.1176  \t val_acc=0.8488  \t time=1.32s\n",
      "Epoch 50/100 \t loss=27.3067 \t val_loss=134.8710  \t val_acc=0.8478  \t time=1.34s\n",
      "Epoch 51/100 \t loss=26.5146 \t val_loss=157.6795  \t val_acc=0.8456  \t time=1.32s\n",
      "Epoch 52/100 \t loss=26.3765 \t val_loss=146.9928  \t val_acc=0.8496  \t time=1.32s\n",
      "Epoch 53/100 \t loss=25.7418 \t val_loss=124.5151  \t val_acc=0.8448  \t time=1.32s\n",
      "Epoch 54/100 \t loss=26.4535 \t val_loss=189.4911  \t val_acc=0.8421  \t time=1.34s\n",
      "Epoch 55/100 \t loss=26.2620 \t val_loss=152.2138  \t val_acc=0.8461  \t time=1.37s\n",
      "Epoch 56/100 \t loss=29.9331 \t val_loss=91.8186  \t val_acc=0.8503  \t time=1.34s\n",
      "Epoch 57/100 \t loss=28.5405 \t val_loss=103.5261  \t val_acc=0.8496  \t time=1.34s\n",
      "Epoch 58/100 \t loss=26.4706 \t val_loss=121.4636  \t val_acc=0.8521  \t time=1.33s\n",
      "Epoch 59/100 \t loss=24.5902 \t val_loss=142.2585  \t val_acc=0.8498  \t time=1.34s\n",
      "Epoch 60/100 \t loss=24.2885 \t val_loss=147.6242  \t val_acc=0.8528  \t time=1.32s\n",
      "Epoch 61/100 \t loss=23.5485 \t val_loss=153.5718  \t val_acc=0.8463  \t time=1.33s\n",
      "Epoch 62/100 \t loss=23.4823 \t val_loss=143.0188  \t val_acc=0.8528  \t time=1.32s\n",
      "Epoch 63/100 \t loss=23.2505 \t val_loss=171.5409  \t val_acc=0.8523  \t time=1.35s\n",
      "Epoch 64/100 \t loss=22.7570 \t val_loss=177.5559  \t val_acc=0.8486  \t time=1.32s\n",
      "Epoch 65/100 \t loss=22.9492 \t val_loss=142.8346  \t val_acc=0.8338  \t time=1.32s\n",
      "Epoch 66/100 \t loss=23.6551 \t val_loss=171.1839  \t val_acc=0.8438  \t time=1.33s\n",
      "Epoch 67/100 \t loss=23.5938 \t val_loss=154.3475  \t val_acc=0.8453  \t time=1.31s\n",
      "Epoch 68/100 \t loss=23.1177 \t val_loss=166.7277  \t val_acc=0.8461  \t time=1.33s\n",
      "Epoch 69/100 \t loss=23.5636 \t val_loss=122.6479  \t val_acc=0.8491  \t time=1.33s\n",
      "Epoch 70/100 \t loss=25.3754 \t val_loss=149.2973  \t val_acc=0.8491  \t time=1.34s\n",
      "Epoch 71/100 \t loss=23.0476 \t val_loss=148.2369  \t val_acc=0.8468  \t time=1.43s\n",
      "Epoch 72/100 \t loss=22.0763 \t val_loss=183.0370  \t val_acc=0.8478  \t time=1.31s\n",
      "Epoch 73/100 \t loss=22.1046 \t val_loss=169.0097  \t val_acc=0.8413  \t time=1.32s\n",
      "Epoch 74/100 \t loss=21.5069 \t val_loss=185.5042  \t val_acc=0.8483  \t time=1.34s\n",
      "Epoch 75/100 \t loss=21.4705 \t val_loss=251.1060  \t val_acc=0.8453  \t time=1.33s\n",
      "Epoch 76/100 \t loss=24.5975 \t val_loss=204.3009  \t val_acc=0.8506  \t time=1.39s\n",
      "Epoch 77/100 \t loss=23.4082 \t val_loss=200.6084  \t val_acc=0.8355  \t time=1.32s\n",
      "Epoch 78/100 \t loss=24.9769 \t val_loss=160.6321  \t val_acc=0.8360  \t time=1.36s\n",
      "Epoch 79/100 \t loss=22.9879 \t val_loss=170.7325  \t val_acc=0.8491  \t time=1.33s\n",
      "Epoch 80/100 \t loss=22.1721 \t val_loss=176.0166  \t val_acc=0.8466  \t time=1.33s\n",
      "Epoch 81/100 \t loss=23.6801 \t val_loss=211.8023  \t val_acc=0.8471  \t time=1.31s\n",
      "Epoch 82/100 \t loss=22.4962 \t val_loss=172.9322  \t val_acc=0.8451  \t time=1.34s\n",
      "Epoch 83/100 \t loss=20.0018 \t val_loss=219.0089  \t val_acc=0.8446  \t time=1.33s\n",
      "Epoch 84/100 \t loss=20.9477 \t val_loss=205.4614  \t val_acc=0.8413  \t time=1.34s\n",
      "Epoch 85/100 \t loss=21.5506 \t val_loss=268.0845  \t val_acc=0.8461  \t time=1.33s\n",
      "Epoch 86/100 \t loss=19.2125 \t val_loss=273.5661  \t val_acc=0.8473  \t time=1.31s\n",
      "Epoch 87/100 \t loss=20.1444 \t val_loss=187.0865  \t val_acc=0.8248  \t time=1.33s\n",
      "Epoch 88/100 \t loss=27.8387 \t val_loss=193.6143  \t val_acc=0.8416  \t time=1.31s\n",
      "Epoch 89/100 \t loss=20.7631 \t val_loss=244.6922  \t val_acc=0.8483  \t time=1.34s\n",
      "Epoch 90/100 \t loss=19.3503 \t val_loss=289.1664  \t val_acc=0.8428  \t time=1.33s\n",
      "Epoch 91/100 \t loss=18.5907 \t val_loss=310.0804  \t val_acc=0.8408  \t time=1.40s\n",
      "Epoch 92/100 \t loss=17.7411 \t val_loss=196.1391  \t val_acc=0.8325  \t time=1.32s\n",
      "Epoch 93/100 \t loss=18.9880 \t val_loss=288.0433  \t val_acc=0.8493  \t time=1.33s\n",
      "Epoch 94/100 \t loss=16.6022 \t val_loss=322.8986  \t val_acc=0.8473  \t time=1.35s\n",
      "Epoch 95/100 \t loss=17.9089 \t val_loss=349.6047  \t val_acc=0.8496  \t time=1.33s\n",
      "Epoch 96/100 \t loss=17.6011 \t val_loss=355.7108  \t val_acc=0.8438  \t time=1.35s\n",
      "Epoch 97/100 \t loss=17.5352 \t val_loss=296.8413  \t val_acc=0.8418  \t time=1.35s\n",
      "Epoch 98/100 \t loss=18.7434 \t val_loss=273.0132  \t val_acc=0.8458  \t time=1.34s\n",
      "Epoch 99/100 \t loss=18.3918 \t val_loss=261.8674  \t val_acc=0.8463  \t time=1.34s\n",
      "Epoch 100/100 \t loss=17.9686 \t val_loss=345.8157  \t val_acc=0.8471  \t time=1.34s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.  \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Predict/Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    # Set model to validation configuration -Doesn't get trained here\n",
    "    model.eval()        \n",
    "    avg_val_loss = 0.\n",
    "    val_preds = np.zeros((len(x_test),len(le.classes_)))\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        # keep/store predictions\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] = nn.functional.softmax(y_pred).cpu().numpy()\n",
    "\n",
    "    # Check Accuracy\n",
    "    val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model/lstm_condition'\n",
    "torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers=1, drop_p = 0.8):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab  \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        #concat of avg_pool and max_pool generate twice the dimension\n",
    "        self.fc = nn.Linear(n_hidden*2, n_hidden//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(n_hidden//2, n_output) \n",
    "        \n",
    "    def forward(self, tokenized_idx):\n",
    "        embeddings = self.embedding(tokenized_idx)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        avg_pool = torch.mean(lstm_out, 1)\n",
    "        max_pool, _ = torch.max(lstm_out, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.fc(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(5001, 64)\n",
       "  (lstm): LSTM(64, 100, num_layers=2, batch_first=True, dropout=0.8)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (fc): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Linear(in_features=50, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = len(w2id_dict)\n",
    "n_embed = 64\n",
    "n_hidden = 100\n",
    "n_output = 6\n",
    "n_layers = 2\n",
    "batch_size = 100\n",
    "\n",
    "model = Model(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pain', 'Anxiety'], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp =['taken it for a long time it is the only thing that works for me',\n",
    " 'tried all the benzodiazepines with no luck then they tried buspar and it changed my life']\n",
    "\n",
    "sent_idx = sent_to_idx(tmp, w2id_dict)\n",
    "sent_idx = torch.tensor(sent_idx, dtype=torch.long)\n",
    "pred = model(sent_idx).detach()\n",
    "pred = nn.functional.softmax(pred, 1).numpy()\n",
    "pred = pred.argmax(axis=1)\n",
    "pred = le.classes_[pred]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
